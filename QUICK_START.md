# Quick Start Guide: Ch·∫°y Experiments

## T·ªïng quan

ƒê·ªÉ c√≥ k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß cho nghi√™n c·ª©u, b·∫°n c·∫ßn ch·∫°y 3 milestones:

1. **Milestone 2**: Deviation Suite (ƒëo Deviation Gain)
2. **Milestone 3**: Baseline Comparison (so s√°nh v·ªõi Self-Consistency, Self-Refine)
3. **Milestone 4**: Protocol Progression (P1 ‚Üí P2 ‚Üí P3)

---

## C√°ch 1: Ch·∫°y T·∫•t C·∫£ (Khuy·∫øn ngh·ªã) üöÄ

Ch·∫°y m·ªôt l·ªánh ƒë·ªÉ c√≥ t·∫•t c·∫£ k·∫øt qu·∫£:

```bash
python run_all_milestones.py
```

**Th·ªùi gian**: ~5-10 ph√∫t (v·ªõi mock LLM ƒë·ªÉ test)

**K·∫øt qu·∫£:**
- `results/milestone2/` - Deviation suite results
- `results/milestone3/` - Baseline comparison
- `results/milestone4/` - Protocol progression

---

## C√°ch 2: Ch·∫°y T·ª´ng Milestone

### Milestone 2: Deviation Suite

Test protocol v·ªõi 6 deviation types (honest, lie, withhold, etc.):

```bash
python run_milestone2_deviation.py
```

**Output:**
- Deviation Gain (DG) cho m·ªói deviation type
- Incentive Robustness Index (IRI)
- % episodes c√≥ DG > 0

**M·ª•c ti√™u:** DG < 0 cho t·∫•t c·∫£ deviations (protocol hi·ªáu qu·∫£)

---

### Milestone 3: Baseline Comparison

So s√°nh P1 v·ªõi baseline m·∫°nh:

```bash
python run_milestone3_baselines.py
```

**Baselines:**
- Self-Consistency (K=5 samples)
- Self-Refine (2 rounds)

**Output:**
- Accuracy comparison
- Evidence compliance (unique value c·ªßa P1)
- Cost efficiency

**M·ª•c ti√™u:** P1 accuracy trong ¬±5% c·ªßa best baseline nh∆∞ng c√≥ evidence compliance >90%

---

### Milestone 4: Protocol Progression

Test P1 ‚Üí P2 ‚Üí P3:

```bash
python run_milestone4_protocols.py
```

**Protocols:**
- P1: Evidence-First
- P2: + Cross-Examination
- P3: + Reputation/Slashing

**Output:**
- IRI cho m·ªói protocol
- Reputation trajectories (P3)
- Protocol progression analysis

**M·ª•c ti√™u:** IRI(P3) < IRI(P2) < IRI(P1) (c·∫£i thi·ªán d·∫ßn)

---

## Y√™u C·∫ßu Tr∆∞·ªõc Khi Ch·∫°y

### 1. C√†i ƒë·∫∑t dependencies

```bash
pip install -r requirements.txt
```

### 2. C·∫•u h√¨nh API key (n·∫øu d√πng LLM th·∫≠t)

T·∫°o file `.env`:

```bash
OPENAI_API_KEY=your-api-key-here
```

**L∆∞u √Ω:** Scripts hi·ªán t·∫°i d√πng mock LLM ƒë·ªÉ test nhanh. ƒê·ªÉ d√πng LLM th·∫≠t, s·ª≠a `model_name="mock"` th√†nh `model_name="gpt-4"` trong c√°c script.

### 3. T·∫°o th∆∞ m·ª•c results

```bash
mkdir -p results/milestone2 results/milestone3 results/milestone4
```

---

## K·∫øt Qu·∫£ S·∫Ω C√≥

### üìä Milestone 2: Deviation Suite Results

**File:** `results/milestone2/deviation_suite_results.json`

```json
{
  "deviation_gains": {
    "lie": {
      "deviation_gain": -0.45,
      "percent_dg_positive": 12.0
    },
    "withhold": {
      "deviation_gain": -0.28,
      "percent_dg_positive": 23.0
    }
  },
  "iri": 0.03
}
```

**Gi·∫£i th√≠ch:**
- `deviation_gain < 0` = Protocol hi·ªáu qu·∫£ (n√≥i d·ªëi kh√¥ng c√≥ l·ª£i)
- `iri < 0.05` = Excellent robustness

---

### üìä Milestone 3: Baseline Comparison

**File:** `results/milestone3/baseline_comparison.json`

```json
{
  "methods": {
    "protocol_p1": {
      "accuracy": 0.68,
      "evidence_compliance": 0.92
    },
    "self_consistency_k5": {
      "accuracy": 0.71,
      "evidence_compliance": 0.0
    },
    "self_refine_r2": {
      "accuracy": 0.69,
      "evidence_compliance": 0.0
    }
  }
}
```

**Key Finding:** P1 competitive accuracy (68% vs 71%) nh∆∞ng l√† method duy nh·∫•t c√≥ evidence compliance cao (92%)

---

### üìä Milestone 4: Protocol Progression

**File:** `results/milestone4/protocol_comparison.json`

```json
{
  "P1_Evidence_First": {"iri": 0.15},
  "P2_Cross_Exam": {"iri": 0.08},
  "P3_Slashing": {"iri": 0.03}
}
```

**Key Finding:** IRI gi·∫£m d·∫ßn P1 ‚Üí P2 ‚Üí P3, ch·ª©ng minh protocol progression

---

## Troubleshooting

### L·ªói: Module not found

```bash
# ƒê·∫£m b·∫£o ch·∫°y t·ª´ th∆∞ m·ª•c g·ªëc
cd /home/admin1/Desktop/MAScheaptalk
python run_all_milestones.py
```

### L·ªói: API key not set

Scripts hi·ªán t·∫°i d√πng mock LLM, kh√¥ng c·∫ßn API key. N·∫øu mu·ªën d√πng LLM th·∫≠t:

1. Set API key trong `.env`
2. S·ª≠a `model_name="mock"` ‚Üí `model_name="gpt-4"` trong scripts

### L·ªói: FEVER dataset kh√¥ng load ƒë∆∞·ª£c

Scripts t·ª± ƒë·ªông t·∫°o mock data n·∫øu FEVER kh√¥ng load ƒë∆∞·ª£c. Mock data ƒë·ªß ƒë·ªÉ test pipeline.

---

## Ch·∫°y V·ªõi LLM Th·∫≠t (Production)

ƒê·ªÉ ch·∫°y v·ªõi OpenAI GPT-4:

1. **Set API key:**
```bash
export OPENAI_API_KEY=your-key
```

2. **S·ª≠a scripts:**
Trong m·ªói script `run_milestone*.py`, thay:
```python
model_name="mock"
```
th√†nh:
```python
model_name="gpt-4"
```

3. **TƒÉng s·ªë tasks:**
S·ª≠a `num_samples=10` th√†nh `num_samples=100` ƒë·ªÉ c√≥ k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß

**Chi ph√≠ ∆∞·ªõc t√≠nh:**
- 100 tasks √ó 6 deviations √ó 3 protocols = 1800 episodes
- ~10-20 USD (t√πy model v√† token usage)

---

## K·∫ø Ho·∫°ch Ch·∫°y ƒê·∫ßy ƒê·ªß

### Phase 1: Test Pipeline (Ngay b√¢y gi·ªù)
```bash
python run_all_milestones.py
```
- D√πng mock LLM
- 10 tasks per milestone
- Ki·ªÉm tra pipeline ho·∫°t ƒë·ªông ƒë√∫ng

### Phase 2: Pilot Run (1-2 gi·ªù)
```bash
# S·ª≠a num_samples=10 ‚Üí 30 trong scripts
python run_all_milestones.py
```
- D√πng GPT-4 ho·∫∑c local model
- 30 tasks ƒë·ªÉ xem trend
- Chi ph√≠: ~5 USD

### Phase 3: Full Run (3-5 gi·ªù)
```bash
# S·ª≠a num_samples=30 ‚Üí 100 trong scripts
python run_all_milestones.py
```
- 100 tasks ƒë·ªß statistical significance
- Chi ph√≠: ~15-20 USD
- K·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß cho paper

---

## S·ª≠ D·ª•ng K·∫øt Qu·∫£ Cho Paper

### Tables cho Paper

**Table 1: Deviation Gain Analysis**
```
Deviation Type    DG       %DG>0   Status
--------------------------------------------
lie              -0.45     12%     ‚úì Effective
withhold         -0.28     23%     ‚úì Effective
persuasion_only  -0.62     5%      ‚úì‚úì Very effective

IRI: 0.03 (Excellent robustness)
```

**Table 2: Baseline Comparison**
```
Method              Accuracy  Evidence   Notes
------------------------------------------------
P1 Evidence-First   68%       92%        Unique value
Self-Consistency    71%       0%         Best accuracy
Self-Refine         69%       0%         Iterative
```

**Table 3: Protocol Progression**
```
Protocol    IRI    Improvement vs P1
------------------------------------
P1          0.15   Baseline
P2          0.08   47% better
P3          0.03   80% better
```

---

## C√¢u H·ªèi Th∆∞·ªùng G·∫∑p

**Q: M·∫•t bao l√¢u ƒë·ªÉ ch·∫°y?**
A: 
- Mock LLM: 5-10 ph√∫t
- GPT-4 (10 tasks): 10-15 ph√∫t
- GPT-4 (100 tasks): 2-3 gi·ªù

**Q: C√≥ c·∫ßn GPU kh√¥ng?**
A: Kh√¥ng, ch·∫°y tr√™n CPU b√¨nh th∆∞·ªùng ƒë∆∞·ª£c.

**Q: K·∫øt qu·∫£ c√≥ reproducible kh√¥ng?**
A: C√≥, v·ªõi seeds c·ªë ƒë·ªãnh (42, 43, 44) trong configs.

**Q: L√†m sao ki·ªÉm tra k·∫øt qu·∫£ ƒë√∫ng?**
A: 
1. Check IRI < 0.15 (good)
2. Check DG < 0 cho majority deviations
3. Check P1 evidence compliance > 80%
4. Check protocol progression P1 < P2 < P3

---

## Next Steps

Sau khi c√≥ k·∫øt qu·∫£:

1. **Analyze**: ƒê·ªçc JSON results
2. **Visualize**: T·∫°o plots (DG, IRI, trajectories)
3. **Write**: D√πng k·∫øt qu·∫£ cho paper sections
4. **Iterate**: Tune parameters n·∫øu c·∫ßn

Good luck! üöÄ
