"""Run Milestone 4: Protocol Comparison (P1, P2, P3) experiments."""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add src to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from infrastructure.datasets.hf_fever_repo import HFFEVERRepository
from infrastructure.verifiers.verifier_factory import create_fever_verifier
from infrastructure.storage.jsonl_storage import JSONLStorage
from infrastructure.storage.reputation_store import ReputationStore
from infrastructure.llm.api_key_pool import APIKeyPool
from infrastructure.storage.checkpoint_manager import CheckpointManager
from domain.entities.payoff import PayoffConfig
from domain.entities.reputation import ReputationConfig
from application.protocols.p1_evidence_first import P1EvidenceFirstProtocol
from application.protocols.p2_cross_exam import P2CrossExamProtocol
from application.protocols.p3_slashing import P3SlashingProtocol
from application.use_cases.run_episode import RunEpisode
from application.use_cases.run_deviation_suite import RunDeviationSuite


def _get_submission_config():
    """Read task/sample/seed from env for submission runs. Default: 500 tasks, seed 42."""
    def _int_env(name: str, default: int) -> int:
        v = os.getenv(name)
        if v is None or v.strip() == "":
            return default
        try:
            return max(1, int(v.strip()))
        except ValueError:
            return default
    return (_int_env("NUM_TASKS", 500), _int_env("NUM_SAMPLES", 500), 
            _int_env("SEED", 42), _int_env("MAX_CONCURRENT_EPISODES", 30),
            _int_env("BATCH_SIZE", 100))


def main():
    """Run Milestone 4: Protocol Comparison."""
    
    # Load environment
    load_dotenv()
    
    num_tasks, num_samples, seed, max_concurrent, batch_size = _get_submission_config()
    
    print("="*80)
    print("MILESTONE 4: PROTOCOL PROGRESSION (P1 â†’ P2 â†’ P3) - PARALLEL")
    print("="*80)
    print("\nThis will compare protocol effectiveness:")
    print("  - P1: Evidence-First")
    print("  - P2: Evidence-First + Cross-Examination")
    print("  - P3: Evidence-First + Cross-Exam + Reputation/Slashing")
    print("\nExpected: P1 < P2 < P3 (increasing robustness)")
    print("\nConfiguration:")
    print(f"  - Tasks: {num_tasks} per protocol (set NUM_TASKS in .env for submission)")
    print(f"  - Samples: {num_samples} (set NUM_SAMPLES in .env)")
    print(f"  - Seed: {seed} (set SEED in .env)")
    print(f"  - Max concurrent: {max_concurrent} (set MAX_CONCURRENT_EPISODES in .env)")
    print(f"  - Batch size: {batch_size} (set BATCH_SIZE in .env)")
    print("  - Model: gpt-4o-mini (cost-effective)")
    print("  - Deviation types: honest, lie, withhold, persuasion_only")
    print("="*80)
    
    # Get API keys (support multiple keys)
    api_keys_str = os.getenv('OPENAI_API_KEYS', os.getenv('OPENAI_API_KEY', ''))
    if not api_keys_str:
        print("\nâŒ ERROR: OPENAI_API_KEY or OPENAI_API_KEYS not set!")
        print("Please set it:")
        print("  export OPENAI_API_KEY=your-key")
        print("  or for multiple keys: export OPENAI_API_KEYS=key1,key2,key3")
        return 1
    
    # Create API key pool
    api_key_pool = APIKeyPool.from_env_string(
        api_keys_str,
        rate_limit_rpm=int(os.getenv('RATE_LIMIT_RPM', '500'))
    )
    print(f"\nâœ“ Using {len(api_key_pool)} API key(s) for load balancing")
    
    # For backwards compatibility
    api_key = api_keys_str.split(',')[0].strip()
    
    # Payoff config
    payoff_config = PayoffConfig(lambda_cost=0.01, mu_penalty=0.5)
    
    # Reputation config (for P3)
    reputation_config = ReputationConfig(
        alpha=0.1,
        floor=0.2,
        threshold=0.5,
        slashing_penalty=0.15,
        redemption_bonus=0.1
    )
    
    # Results storage
    all_results = {}
    
    # Test each protocol
    protocols = [
        ("P1_Evidence_First", P1EvidenceFirstProtocol()),
        ("P2_Cross_Exam", P2CrossExamProtocol(num_cross_exam_questions=2)),
        ("P3_Slashing", P3SlashingProtocol(
            num_cross_exam_questions=2,
            reputation_config=reputation_config,
            reputation_store=ReputationStore("data/reputation_test.db")
        ))
    ]
    
    deviation_types = ["honest", "lie", "withhold", "persuasion_only"]
    
    for protocol_name, protocol in protocols:
        print("\n" + "="*80)
        print(f"TESTING: {protocol_name}")
        print("="*80)
        
        # Initialize components
        print(f"\n[1/4] Initializing dataset...")
        dataset_repo = HFFEVERRepository(
            split="dev",  # FEVER uses 'dev' not 'validation'
            num_samples=num_samples,
            seed=seed
        )
        
        print(f"[2/4] Initializing verifier (evidence: string match + optional LLM eval)...")
        verifier = create_fever_verifier(
            use_semantic_matching=False,
            use_llm_evidence_eval=None,
            llm_model="gpt-4o-mini",
            api_key=api_key,
        )
        
        print(f"[3/4] Initializing storage...")
        storage_path = f"results/milestone4/{protocol_name.lower()}_episodes.jsonl"
        storage = JSONLStorage(filepath=storage_path)
        
        # Checkpoint manager for this protocol
        checkpoint_manager = CheckpointManager(
            checkpoint_dir="results/checkpoints",
            milestone=f"milestone4_{protocol_name}",
            seed=seed,
            auto_cleanup=True
        )
        checkpoint_manager.load()
        
        print(f"[4/4] Running deviation suite...")
        
        # Run episode with API key pool
        run_episode = RunEpisode(
            protocol=protocol,
            verifier=verifier,
            model_name="gpt-4o-mini",
            api_key=api_key,
            api_key_pool=api_key_pool,
            lambda_cost=0.01,
            mu_penalty=0.5
        )
        
        # Deviation suite
        deviation_suite = RunDeviationSuite(
            run_episode=run_episode,
            dataset_repo=dataset_repo,
            storage=storage,
            num_tasks=num_tasks,
            deviation_types=deviation_types,
            payoff_config=payoff_config,
            max_concurrent=max_concurrent,
            batch_size=batch_size,
            checkpoint_manager=checkpoint_manager
        )
        
        try:
            results = deviation_suite.execute_sync()
            all_results[protocol_name] = results
            
            print(f"\nâœ“ {protocol_name} complete")
            print(f"  IRI: {results['iri']:.3f}")
            
        except Exception as e:
            print(f"\nâŒ Error testing {protocol_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # Print comparison
    print("\n" + "="*80)
    print("PROTOCOL PROGRESSION ANALYSIS")
    print("="*80)
    
    print(f"\n{'Protocol':<20} {'IRI':>10} {'Evidence (honest)':<18} {'Status':<20}")
    print("-"*70)
    
    for protocol_name, results in all_results.items():
        iri = results['iri']
        honest_m = results.get("metrics_by_type", {}).get("honest", {})
        ev_match = honest_m.get("evidence_match_score", 0)
        ev_compl = honest_m.get("evidence_compliance", 0)
        ev_str = f"match={ev_match:.2f} compl={ev_compl:.0%}"
        if iri < 0.05:
            status = "âœ“âœ“ Excellent"
        elif iri < 0.15:
            status = "âœ“ Good"
        else:
            status = "â—‹ Moderate"
        print(f"{protocol_name:<20} {iri:>10.3f} {ev_str:<18} {status:<20}")
    
    # Check progression
    print("\nðŸ“Š PROGRESSION CHECK:")
    iri_values = [results['iri'] for results in all_results.values()]
    
    if len(iri_values) >= 2:
        if iri_values[-1] < iri_values[0]:
            improvement = ((iri_values[0] - iri_values[-1]) / iri_values[0]) * 100
            print(f"  âœ“ P3 improved {improvement:.1f}% over P1")
        else:
            print(f"  âš  No clear progression detected")
    
    # Reputation summary (for P3)
    if "P3_Slashing" in all_results:
        print("\nðŸ“ˆ REPUTATION SUMMARY (P3):")
        p3_protocol = protocols[2][1]  # P3 protocol instance
        p3_protocol.print_reputation_summary()
    
    # Print API key pool stats
    if len(api_key_pool) > 1:
        api_key_pool.print_stats()
    
    # Save combined results
    import json
    output_path = "results/milestone4/protocol_comparison.json"
    os.makedirs("results/milestone4", exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(all_results, f, indent=2, default=str)
    
    print("\n" + "="*80)
    print("âœ… MILESTONE 4 COMPLETE")
    print("="*80)
    print(f"\nResults saved to: {output_path}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
